class Description():
  desc = dict({
    'desc_0': """
<h3>Continuous</h3>
<p>Continuous data is a type of numerical data that refers to the unspecified number of possible measurements between two realistic points.</p>
<p>These numbers are not always clean and tidy like those in discrete data, as they're usually collected from precise measurements. Over time, measuring a particular subject allows us to create a defined range, where we can reasonably expect to collect more data.</p>
<p>Continuous data is all about accuracy. Variables in these data sets often carry decimal points, with the number to the right stretched out as far as possible. This level of detail is paramount for scientists, doctors, and manufacturers, to name a few.</p>
<p>Some examples of continuous data include:</p>
<ul>
  <li>The weight of new born babies</li>
  <li>The daily wind speed</li>
  <li>The temperature of a freezer</li>
</ul>
<p>When you think of experiments or studies involving constant measurements, they're likely to be continuous variables to some extent. If you have a number like “2.86290” anywhere on a spreadsheet, it's not a number you could have quickly arrived at yourself — think measurement devices like stopwatches, scales, thermometers, and the like.</p>

<h3>Discrete, Categorical</h3>
<p>Discrete data is a numerical type of data that includes whole, concrete numbers with specific and fixed data values determined by counting. Continuous data includes complex numbers and varying data values that are measured over a specific time interval.</p>
<p>Some examples of discrete data one might gather:</p>
<ul>
  <li>The number of customers who bought different items</li>
  <li>The number of computers in each department</li>
  <li>The number of items you buy at the grocery store each week</li>
</ul>
<p>Discrete data can also be qualitative. The nationality you select on a form is a piece of discrete data. The nationalities of everyone in your workplace, when grouped, can be valuable information in evaluating your hiring practices.</p>
    """,
    'desc_1': """
<h3>Relationships</h3>
<p>Relationships in probability and statistics can generally be one of three things: deterministic, random, or statistical. Relationship has been divided by 3 types of relationship.</p>
<ul>
  <li><b>Deterministic Relationship</b></li>
  <p>A deterministic relationship involves an exact relationship between two variables.</p>
  <li><b>Random Relationship</b></li>
  <p>A random relationship is a bit of a misnomer because there is no relationship between the variables. However, random processes may make it seem like there is a relationship.</p>
  <li><b>Statistical Relationship</b></li>
  <p>A statistical relationship is a mixture of deterministic and random relationships.</p>
</ul>
<h3>Differences</h3>
<p>
Statistical difference refers to significant differences between groups of objects or people. Scientists calculate this difference to determine whether the data from an experiment is reliable before drawing conclusions and publishing results. When studying the relationship between two variables, scientists use the chi-square calculation method. When comparing two groups, scientists use the t-distribution method
</p>
<ul>
<li><b>Chi-Square Method</b></li>
<p>Create a data table with a row for each possible result and a column for each group involved in the experiment. For example, if you are trying to answer the question of whether picture flash cards or word flash cards better help children pass a vocabulary test, you would create a table with three columns and two rows. The first column would be marked, "Passed Test?" and two rows beneath the heading would be marked "Yes" and "No." The next column would be labelled "Picture Cards" and the final column would be labelled "Word Cards."</p>
<li><b>T-Test Method</b></li>
<p>Make a data table showing the number of observations for each of two groups, the mean of the results for each group, the standard deviation from each mean and the variance for each mean. Subtract the group two mean from the group one mean. Divide each variance by the number of observations minus 1. For example, if one group had a variance of 2186753 and 425 observations, you would divide 2186753 by 424. Take the square root of each result. Divide each result by the corresponding result from Step 2. Calculate the degrees of freedom by totalling the number of observations for both groups and dividing by 2.</p>
</ul>
    """,
    'desc_11': """
<h3>Independent variable?</h3>
<p><i>The two main variables in an experiment are the independent and dependent variable.</i></p>
<p>An independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on the dependent variable. A dependent variable is the variable being tested and measured in a scientific experiment.</p>
<p><i>What's the Difference Between Homogeneous and Heterogeneous?</i></p>
<p>The dependent variable is 'dependent' on the independent variable. As the experimenter changes the independent variable, the effect on the dependent variable is observed and recorded.</p>
<p><i>Independent and Dependent Variable Example.</i></p>
<p>For example, a scientist wants to see if the brightness of light has any effect on a moth being attracted to the light. The brightness of the light is controlled by the scientist. This would be the independent variable. How the moth reacts to the different light levels (distance to light source) would be the dependent variable.</p>
<p><i>How to Tell the Variables Apart.</i></p>
<p>The independent and dependent variables may be viewed in terms of cause and effect. If the independent variable is changed, then an effect is seen in the dependent variable. Remember, the values of both variables may change in an experiment and are recorded. The difference is that the value of the independent variable is controlled by the experimenter, while the value of the dependent variable only changes in response to the independent variable.</p>
    """,
    'desc_12': """
<h3>Means</h3>
<p>In statistics, the mean summarizes an entire dataset with a single number representing the data's center point or typical value. It is also known as the arithmetic average, and it is one of several measures of central tendency.</p>
<p>Ideally, the mean indicates the region where most values in a distribution fall. Statisticians refer to it as the central location of a distribution. You can think of it as the tendency of data to cluster around a middle value.</p>

<h3>Tests for Equal Variances</h3>
<p>Use a test for equal variances to test the equality of variances between populations or factor levels. Many statistical procedures, such as analysis of variance (ANOVA) and regression, assume that although different samples can come from populations with different means, they have the same variance.</p>
<p>Because the susceptibility of different procedures to unequal variances varies greatly, so does the need to do a test for equal variances. For example, ANOVA inferences are only slightly affected by inequality of variance if the model contains only fixed factors and has equal or almost equal sample sizes. Alternatively, ANOVA models with random effects and/or unequal sample sizes could be substantially affected.</p>
<p>For example, you plan to do an ANOVA testing the length of time callers are put on hold where the main fixed factor is the calling center. You use the ANOVA general linear model (GLM) because you have unequal sample sizes. Because this unbalanced condition increases the susceptibility to unequal variances, you decide to test the assumption of equal variances. If the resulting p-value is greater than adequate choices of alpha, you fail to reject the null hypothesis of the variances being equal. You can feel confident that the assumption of equal variances is being met.</p>
<p>For tests for equal variances, the hypotheses are:</p>
<ul>
  <li>H0: All variances are equal</li>
  <li>H1: Not all variances are equal</li>
</ul>
    """,
    'desc_112': """
<h3>Correlation Analysis</h3>
<p>Correlation Analysis is statistical method that is used to discover if there is a relationship between two variables/datasets, and how strong that relationship may be.</p>
<p>In terms of market research this means that, correlation analysis is used to analyse quantitative data gathered from research methods such as surveys and polls, to identify whether there is any significant connections, patterns, or trends between the two.</p>
<p>Essentially, correlation analysis is used for spotting patterns within datasets. A positive correlation result means that both variables increase in relation to each other, while a negative correlation means that as one variable decreases, the other increases.</p>

<h3>Parametric</h3>
<p><i>(Pearson's Coefficient)</i> Where the data must be handled in relation to the parameters of populations or probability distributions. Typically used with quantitative data already set out within said parameters.</p>
<p>Advantage of Parametric:</p>
<ul>
  <li>Parametric tests can provide trustworthy results with distributions that are skewed and nonnormal</li>
  <li>Parametric tests can provide trustworthy results when the groups have different amounts of variability</li>
  <li>Parametric tests have greater statistical power</li>
</ul>

<h3>Nonparametric</h3>
<p><i>(Spearman's Rank)</i> Where no assumptions can be made about the probability distribution. Typically used with qualitative data but can be used with quantitative data if Spearman's Rank proves inadequate.</p>
<p>Advantage of Nonparametric:</p>
<ul>
  <li>Nonparametric tests assess the median which can be better for some study areas</li>
  <li>Nonparametric tests are valid when our sample size is small, and your data are potentially nonnormal</li>
  <li>Nonparametric tests can analyse ordinal data, ranked data, and outliers</li>
</ul>
<p>If we have a small dataset, the distribution can be a deciding factor. However, in many cases, this issue is not critical because of the following:</p>
<ul>
  <li>Parametric analyses can analyse nonnormal distributions for many datasets.</li>
  <li>Nonparametric analyses have other firm assumptions that can be harder to meet.</li>
</ul>
<p>The answer is often contingent upon whether the mean or median is a better measure of central tendency for the distribution of your data.</p>
<ul>
  <li>If the mean is a better measure and you have a sufficiently large sample size, a parametric test usually is the better, more powerful choice.</li>
  <li>If the median is a better measure, consider a nonparametric test regardless of your sample size.</li>
</ul>
    """,
    'above_this': 'is from hidir',
    'below_this': 'is from google',
    'desc_111': """
<h3>Regression Analysis</h3>
<p>
    Regression analysis is a set of statistical methods used for the estimation of
    relationships between a dependent variable and one or more independent variables.
    It can be utilized to assess the strength of the relationship
    between variables and for modeling the future relationship between them.
    Regression analysis includes several variations, such as linear, multiple linear, and nonlinear.
    The most common models are simple linear and multiple linear.
    Nonlinear regression analysis is commonly used for more complicated data sets
    in which the dependent and independent variables show a nonlinear relationship.
</p>
    """,
    'desc_1121': """
<h3>Pearson's R</h3>
<p>
    In Statistics, the Pearson's Correlation Coefficient is also referred to as Pearson's r,
    the Pearson product-moment correlation coefficient (PPMCC), or bivariate correlation.
    It is a statistic that measures the linear correlation between two variables.
    Like all correlations, it also has a numerical value that lies between -1.0 and +1.0.
    Pearson's correlation coefficient is the covariance of the two variables divided by
    the product of their standard deviations. The form of the definition involves a
    "product moment", that is, the mean (the first moment about the origin) of
    the product of the mean-adjusted random variables; hence the modifier product-moment in the name.
    Pearson's Correlation Coefficient is named after Karl Pearson.
    He formulated the correlation coefficient from a related idea by Francis Galton in the 1880s.
</p>
    """,
    'desc_1122': """
<h3>Spearman's Rank Correlation</h3>
<p>
    The Spearman's Rank Correlation Coefficient is used to discover the strength of
    a link between two sets of data. This example looks at the strength of
    the link between the price of a convenience item (a 50cl bottle of water)
    and distance from the Contemporary Art Museum in El Raval, Barcelona.
</p>
    """,
    'desc_121': """
<h3>Fmax Test</h3>
<p>
    The Fmax test (also called Hartley's Fmax) is a test for
    homogenity of variance. In other words, the spread (variance)
    of your data should be similar across groups or levels.
    Compared to Levene's test, Hartley's test is fairly simple
    to figure out by hand. An assumption of the Fmax test is that
    there are an equal number of participants in each group.
</p>
<h3>Brown & Smythe's Test</h3>
<p>
    The Brown-Forsythe test is a statistical test for the equality
    of group variances based on performing an Analysis of Variance
    (ANOVA) on a transformation of the response variable.
    When a one-way ANOVA is performed, samples are assumed to
    have been drawn from distributions with equal variance.
    If this assumption is not valid, the resulting F-test is invalid.
    The Brown-Forsythe test statistic is the F statistic resulting
    from an ordinary one-way analysis of variance on the absolute
    deviations of groups/treatments data from their individual medians.
</p>
    """,
    'desc_122': """
<h3>Two groups</h3>
<p>
    Independent sample t-test is a statistical technique that is
    used to analyze the mean comparison of two independent groups.
    In independent samples t-test, when we take two samples from
    the same population, then the mean of the two samples may be
    identical. But when samples are taken from two different
    populations, then the mean of the sample may differ. In this
    case, it is used to draw conclusions about the means of two
    populations, and used to tell whether or not they are similar.
</p>
<h3>More than two groups</h3>
<p>
    For a comparison of more than two group means the one-way analysis
    of variance (ANOVA) is the appropriate method instead of the t test.
    As the ANOVA is based on the same assumption with the t test,
    the interest of ANOVA is on the locations of the distributions
    represented by means too. Then why is the method comparing several
    means the 'analysis of variance', rather than 'analysis of means'
    themselves? It is because that the relative location of the several
    group means can be more conveniently identified by variance among
    the group means than comparing many group means directly when
    number of means are large.
</p>
    """,
    'desc_1221': """
<h3>Parametric assumptions satisfied</h3>
<p>
    In statistical analysis, all parametric tests assume some certain
    characteristic about the data, also known as assumptions.  Violation
    of these assumptions changes the conclusion of the research and
    interpretation of the results. Therefore all research, whether for
    a journal article, thesis, or dissertation, must follow these
    assumptions for accurate interpretation  Depending on the
    parametric analysis, the assumptions vary.
</p>
    """,
    'desc_12211': """
<h3>Unpaired T-Test</h3>
<p>
    An unpaired t-test (also known as an independent t-test) is a
    statistical procedure that compares the averages/means of
    two independent or unrelated groups to determine if there
    is a significant difference between the two.
</p>
<h3>Paired T-Test</h3>
<p>
    A paired t-test (also known as a dependent or correlated t-test)
    is a statistical test that compares the averages/means
    and standard deviations of two related groups to determine
    if there is a significant difference between the two groups.
</p>
    """,
    'desc_12212': """
<h3>Data Transform</h3>
<p>
    Data Transformation in a statistics context means the application
    of a mathematical expression to each point in the data. In contrast,
    in a Data Engineering context Transformation can also mean transforming
    data from one format to another in the Extract Transform Load (ETL) process.
</p>
    """,
    'desc_122121': """
<h3>Unpaired T-Test</h3>
<p>
    An unpaired t-test (also known as an independent t-test) is a
    statistical procedure that compares the averages/means of
    two independent or unrelated groups to determine if there
    is a significant difference between the two.
</p>
<h3>Paired T-Test</h3>
<p>
    A paired t-test (also known as a dependent or correlated t-test)
    is a statistical test that compares the averages/means
    and standard deviations of two related groups to determine
    if there is a significant difference between the two groups.
</p>
    """,
    'desc_122122': """
<h3>Mann-Whitney U</h3>
<p>
    The Mann-Whitney U test is used to compare differences between two
    independent groups when the dependent variable is either ordinal or
    continuous, but not normally distributed. For example, you could use
    the Mann-Whitney U test to understand whether attitudes towards pay
    discrimination, where attitudes are measured on an ordinal scale,
    differ based on gender (i.e., your dependent variable would be
    "attitudes towards pay discrimination" and your independent variable
    would be "gender", which has two groups: "male" and "female").
</p>
<h3>Wilcoxon Rank Sums Test</h3>
<p>
    The Wilcoxon test, which can refer to either the Rank Sum test
    or the Signed Rank test version, is a nonparametric statistical
    test that compares two paired groups. The tests essentially
    calculate the difference between sets of pairs and analyzes
    these differences to establish if they are statistically
    significantly different from one another.
</p>
    """,
    'desc_1222': """
<h3>Parametric assumptions satisfied</h3>
<p>
    In statistical analysis, all parametric tests assume some certain
    characteristic about the data, also known as assumptions.  Violation
    of these assumptions changes the conclusion of the research and
    interpretation of the results. Therefore all research, whether for
    a journal article, thesis, or dissertation, must follow these
    assumptions for accurate interpretation  Depending on the
    parametric analysis, the assumptions vary.
</p>
    """,
    'desc_12221': """
<h3>ANOVA Test</h3>
<p>
    Analysis of variance (ANOVA) is an analysis tool used in statistics
    that splits an observed aggregate variability found inside a data
    set into two parts: systematic factors and random factors.
    The systematic factors have a statistical influence on the given data set,
    while the random factors do not. Analysts use the ANOVA test to
    determine the influence that independent variables have
    on the dependent variable in a regression study.
</p>
<h3>Tukey's Test</h3>
<p>
    The Tukey Test (or Tukey procedure), also called Tukey's Honest Significant
    Difference test, is a post-hoc test based on the studentized range
    distribution. An ANOVA test can tell you if your results are significant
    overall, but it won't tell you exactly where those differences lie.
    After you have run an ANOVA and found significant results,
    then you can run Tukey's HSD to find out which specific
    groups's means (compared with each other) are different.
    The test compares all possible pairs of means.
</p>
<h3>Bonferroni's Test</h3>
<p>
    The Bonferroni test is a type of multiple comparison test used
    in statistical analysis. When performing a hypothesis test with
    multiple comparisons, eventually a result could occur that appears
    to demonstrate statistical significance in the
    dependent variable, even when there is none.
</p>
    """,
    'desc_12222': """
<h3>Data transform</h3>
<p>
    Data Transformation in a statistics context means the application
    of a mathematical expression to each point in the data. In contrast,
    in a Data Engineering context Transformation can also mean transforming
    data from one format to another in the Extract Transform Load (ETL) process.
</p>
    """,
    'desc_122221': """
<h3>ANOVA Test</h3>
<p>
    Analysis of variance (ANOVA) is an analysis tool used in statistics
    that splits an observed aggregate variability found inside a data
    set into two parts: systematic factors and random factors.
    The systematic factors have a statistical influence on the given data set,
    while the random factors do not. Analysts use the ANOVA test to
    determine the influence that independent variables have
    on the dependent variable in a regression study.
</p>
<h3>Tukey's Test</h3>
<p>
    The Tukey Test (or Tukey procedure), also called Tukey's Honest Significant
    Difference test, is a post-hoc test based on the studentized range
    distribution. An ANOVA test can tell you if your results are significant
    overall, but it won't tell you exactly where those differences lie.
    After you have run an ANOVA and found significant results,
    then you can run Tukey's HSD to find out which specific
    groups's means (compared with each other) are different.
    The test compares all possible pairs of means.
</p>
<h3>Bonferroni's Test</h3>
<p>
    The Bonferroni test is a type of multiple comparison test used
    in statistical analysis. When performing a hypothesis test with
    multiple comparisons, eventually a result could occur that appears
    to demonstrate statistical significance in the
    dependent variable, even when there is none.
</p>
    """,
    'desc_122222': """
<h3>Kruskal-Wallis Test</h3>
<p>
    The Kruskal Wallis test is the non parametric alternative to the
    One Way ANOVA. Non parametric means that the test doesn't assume
    your data comes from a particular distribution. The H test is
    used when the assumptions for ANOVA aren't met (like the assumption
    of normality). It is sometimes called the one-way ANOVA on ranks,
    as the ranks of the data values are used in the
    test rather than the actual data points.
</p>
<h3>Dunn's Test</h3>
<p>
    Dunn's test is a non-parametric pairwise multiple comparisons
    procedure based on rank sums, often used as a post hoc procedure
    following rejection of a Kruskal-Wallis test. As such,
    it is a non-parametric analog to multiple pairwise t tests
    following rejection of an ANOVA null hypothesis.
</p>
    """,
    'desc_2': """
<h3>Chi-Square Tests One and Two Sample</h3>
<p>
    A Chi-square test is a hypothesis testing method.
    Two common Chi-square tests involve checking if observed
    frequencies in one or more categories match expected frequencies.
    You use a Chi-square test for hypothesis tests about whether your
    data is as expected. The basic idea behind the test is to compare
    the observed values in your data to the expected values
    that you would see if the null hypothesis is true.
</p>
    """,
  })
